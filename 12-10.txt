ICLR AMPNET 动态神经网络的异步模型并行训练
动态神经网络中，管理minibatches以保持GPU的充分利用通常被认为是用户的责任

假象有一个 内存开销低，允许完美的缩放而不进行批处理 的设备

前提：不需要分批（完美缩放)  单个内存放不下不分批的数据（需要并行）
提出了AMP算法  异步并行模型算法

 然而，如果我们在完全正向和反向传播之后执行同步参数更新，
	那么增加设备利用率的唯一方法是
	通过将多个实例导入系统
	 由于参数更新频率降低，与同步更新的管道并行性与收敛速度不一致

我们的目标是高设备利用率和更新频率。 
然而，在这种设置中，模型参数可以在实例的正向计算和反向计算之间更新，
引入梯度“时间性”。 
表明AMP训练可以快速收敛

工作：

> 提出了一种异步模型并行训练算法，用于动态网络的高效分布式训练。

> 提出了一个中间表示(IR)，它具有支持AMP训练的分支和连接控制流的显式结构。

> 实证证明AMP训练收敛于与同步算法相似的精度
>
> 总之，我们的工作证明了AMP训练的好处，并给出了一种新的方法来设计和部署具有动态控制流的神经网络库

树结构神经网络 是用于解析自然语言和图像、语义表示和情感分析的强大模型

图神经网络结合了结构上的时间复发和复发



异步模型并行训练